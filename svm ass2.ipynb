{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfba58fa-0b51-4e11-a15a-fd40e69c5f9e",
   "metadata": {},
   "source": [
    "#Q1.\n",
    "\n",
    "Polynomial functions and kernel functions in machine learning are related through the use of kernel methods. Kernel functions play a crucial role in transforming the feature space, allowing linear algorithms to operate effectively in high-dimensional spaces. Polynomial functions are a specific type of kernel function used to introduce non-linearity into these algorithms.\n",
    "\n",
    "Here's the relationship between polynomial functions and kernel functions:\n",
    "\n",
    "    Kernel Functions in General:\n",
    "        Kernel functions are used in machine learning to map data into a higher-dimensional space without explicitly calculating the transformation.\n",
    "        They are used primarily in Support Vector Machines (SVM) and other algorithms, such as kernelized versions of principal component analysis (PCA) and ridge regression.\n",
    "\n",
    "    Polynomial Kernel:\n",
    "        A polynomial kernel is a specific type of kernel function.\n",
    "        It applies a polynomial transformation to the feature vectors. For example, a quadratic (degree-2) polynomial kernel will map data to a feature space with all pairs of features, introducing non-linearity.\n",
    "        The polynomial kernel is defined as K(x, x') = (x · x' + c)^d, where \"d\" is the degree of the polynomial and \"c\" is a constant.\n",
    "\n",
    "    Non-Linearity:\n",
    "        Polynomial kernels are used to capture non-linear relationships in data.\n",
    "        In cases where data is not linearly separable, a linear SVM using a polynomial kernel can find a decision boundary in the higher-dimensional space.\n",
    "\n",
    "    Effect on Decision Boundaries:\n",
    "        The choice of the polynomial degree \"d\" in the kernel function determines the complexity of the decision boundary.\n",
    "        A low degree (e.g., d=1) results in a simple linear boundary, while a higher degree (e.g., d=2 or d=3) can capture more complex, non-linear boundaries.\n",
    "\n",
    "In summary, polynomial functions are a specific type of kernel function used to introduce non-linearity into machine learning algorithms. Kernel functions, including the polynomial kernel, are instrumental in transforming the feature space to address non-linear problems and are commonly used in algorithms like SVM to find optimal decision boundaries in high-dimensional spaces. The degree of the polynomial determines the level of non-linearity introduced into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5194dda9-2751-451e-a9d0-819cccc3fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q2.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (Iris dataset as an example)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train an SVM with a polynomial kernel\n",
    "# You can specify the degree of the polynomial and the C parameter.\n",
    "# The degree controls the complexity of the decision boundary.\n",
    "# The C parameter controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "svm = SVC(kernel='poly', degree=3, C=1.0)  # Example: Polynomial kernel of degree 3\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40e4da-c959-4417-8cb2-09522c19845a",
   "metadata": {},
   "source": [
    "#Q3.\n",
    "\n",
    "In Support Vector Regression (SVR), the value of epsilon (ε) is a hyperparameter that determines the width of the ε-tube or ε-insensitive tube around the regression line. This tube is used to control the trade-off between model accuracy and the number of support vectors. The width of the ε-tube defines the tolerance for errors in SVR.\n",
    "\n",
    "Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "    Smaller Epsilon (Tight ε-Tube):\n",
    "        A smaller epsilon (e.g., ε = 0.1) creates a tight ε-tube around the regression line.\n",
    "        The SVR model becomes very strict and allows very little error. Data points must be close to the regression line or within the ε-tube to be considered support vectors.\n",
    "        This can lead to a smaller number of support vectors, as only data points very close to the regression line are considered.\n",
    "\n",
    "    Larger Epsilon (Wider ε-Tube):\n",
    "        Increasing the value of epsilon (e.g., ε = 1.0 or higher) creates a wider ε-tube.\n",
    "        The SVR model becomes more tolerant of errors. Data points can deviate further from the regression line and still be considered support vectors if they fall within the ε-tube.\n",
    "        This typically results in a larger number of support vectors, as more data points are considered when defining the ε-tube.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR results in a wider ε-tube, making the model more tolerant of errors. This often leads to a larger number of support vectors, as more data points are allowed to be within the ε-tube. The choice of epsilon should be made based on the specific problem, considering the balance between model accuracy and complexity. A smaller epsilon may lead to a more complex model with better fit to training data, while a larger epsilon may lead to a simpler model with broader tolerance for deviations from the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f693-0431-42c8-8c3a-77f7c1e0d05d",
   "metadata": {},
   "source": [
    "#Q4.\n",
    "\n",
    "Support Vector Regression (SVR) is a powerful machine learning algorithm for regression tasks. Several hyperparameters influence the performance of SVR, including the choice of kernel function, the C parameter, the epsilon parameter (ε), and the gamma parameter (for certain kernels). Each parameter serves a specific purpose in controlling the trade-off between model complexity and accuracy. Here's how these parameters work and when you might want to adjust their values:\n",
    "\n",
    "    Choice of Kernel Function:\n",
    "        The kernel function specifies the type of transformation applied to the input features to map them into a higher-dimensional space. Common kernel functions in SVR include Linear, Polynomial, RBF (Radial Basis Function), and Sigmoid.\n",
    "        When to use which kernel:\n",
    "            Linear Kernel: Use for linear relationships between features and the target variable. It provides a simpler, interpretable model.\n",
    "            Polynomial Kernel: Use when the relationship is non-linear, and you need to introduce polynomial interactions between features.\n",
    "            RBF Kernel: A versatile choice for capturing complex, non-linear relationships. It is effective for a wide range of problems.\n",
    "            Sigmoid Kernel: Use for problems that exhibit sigmoidal relationships.\n",
    "\n",
    "    C Parameter:\n",
    "        The C parameter controls the trade-off between maximizing the margin and minimizing the ε-insensitive loss (deviation from the regression line). A smaller C results in a larger margin and allows more errors (larger ε), while a larger C penalizes errors more heavily and aims for a smaller margin.\n",
    "        When to adjust C:\n",
    "            Increase C when you want a more accurate fit to the training data, accepting a smaller margin and fewer errors (potentially more support vectors).\n",
    "            Decrease C when you want a larger margin and can tolerate more errors in the training data.\n",
    "\n",
    "    Epsilon Parameter (ε):\n",
    "        The epsilon parameter defines the width of the ε-insensitive tube. Data points within this tube are considered correctly predicted and do not contribute to the loss. Data points outside the tube contribute to the loss.\n",
    "        When to adjust ε:\n",
    "            Increase ε when you want to tolerate larger prediction errors and accept a wider tube. This results in a more flexible model with potential for larger deviations.\n",
    "            Decrease ε when you want a more precise fit and need to limit the tolerance for errors. This leads to a narrower tube and potentially fewer support vectors.\n",
    "\n",
    "    Gamma Parameter (for RBF Kernel):\n",
    "        The gamma parameter defines the shape and spread of the RBF kernel. A smaller gamma leads to a broader kernel, while a larger gamma results in a more concentrated kernel.\n",
    "        When to adjust gamma:\n",
    "            Increase gamma when you suspect the relationship between input features and the target variable is localized or requires fine-grained modeling.\n",
    "            Decrease gamma when the relationship is more global and smoother.\n",
    "\n",
    "It's essential to adjust these parameters carefully and use techniques like cross-validation to find the best combination for your specific regression problem. Over-optimizing or using inappropriate parameter values can lead to overfitting or poor generalization. Experiment with different values, analyze the trade-offs, and choose the parameters that lead to the best balance between model complexity and accuracy for your particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98e1731-7a3a-49a2-8025-1a54ccae71e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Hyperparameters: {'C': 10, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['iris_svc_classifier.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create and train the SVC classifier\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict labels on the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 7: Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'iris_svc_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc3d88-6216-47ea-9bd7-148b22bd3e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a736905-47bf-4924-8dd6-2758ea8bf319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
